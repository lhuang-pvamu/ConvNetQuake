#!/usr/bin/env python
# encoding: utf-8
# -------------------------------------------------------------------
# File:    train.py
# Author:  Michael Gharbi <gharbi@mit.edu>
# Created: 2016-10-25
# -------------------------------------------------------------------
# 
# 
# 
# ------------------------------------------------------------------#
"""Train a model."""

import argparse
import os
import time

import numpy as np
import tensorflow as tf
import setproctitle

import quakenet.models as models
import quakenet.data_pipeline as dp
import quakenet.config as config
import quakenet.Keras_models as k_models
from tensorflow.keras.callbacks import TensorBoard
import quakenet.data_generator as dg
from tensorflow.keras.models import model_from_json

def main(args):
  _EPOCHS = 5
  setproctitle.setproctitle('quakenet')

  tf.set_random_seed(1234)

  if args.n_clusters == None:
    raise ValueError('Define the number of clusters with --n_clusters')

  cfg = config.Config()
  cfg.batch_size = args.batch_size
  cfg.add = 1
  cfg.n_clusters = args.n_clusters
  cfg.n_clusters += 1

  pos_path = os.path.join(args.dataset,"positive")
  neg_path = os.path.join(args.dataset,"negative")
  print(pos_path, neg_path)

  # data pipeline for positive and negative examples
  #pos_pipeline = dp.DataPipeline(pos_path, cfg, True)
  # neg_pipeline = dp.DataPipeline(neg_path, cfg, True)
  #
  # pos_samples = {
  #   'data': pos_pipeline.samples,
  #   'cluster_id': pos_pipeline.labels
  #   }
  # neg_samples = {
  #   'data': neg_pipeline.samples,
  #   'cluster_id': neg_pipeline.labels
  #   }
  #
  # samples = {
  #   "data": tf.concat([pos_samples["data"],neg_samples["data"]], 0),
  #   "cluster_id" : tf.concat([pos_samples["cluster_id"],neg_samples["cluster_id"]],0)
  #   }

  # model
  #model = models.get(args.model, samples,cfg, args.checkpoint_dir, is_training=True)

  dataGenerator = dg.DataGenerator([pos_path,neg_path],cfg, is_training=True)
  dataset = dataGenerator.read().dataset

  if_reuse = True

  if if_reuse and os.path.exists("models/model.json"):
    # load json and create model
    json_file = open("models/model.json", 'r')
    loaded_model_json = json_file.read()
    json_file.close()
    model = model_from_json(loaded_model_json)
    # load weights into new model
    model.load_weights("models/model.h5")
    print("Loaded model from disk")
  else:
    MyModel = k_models.KerasConvNetQuake(cfg,args.checkpoint_dir, is_training=True)
    MyModel.set_model()
    model = MyModel.model
    print("Create an empty model")
    #opt = tf.keras.optimizers.RMSprop(lr=0.0001, decay=1e-6)

  tensorboard = TensorBoard(log_dir="output/logs")
  model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])
  model.fit(
      dataset.make_one_shot_iterator(),
      steps_per_epoch= 10000, #len(dataGenerator) // cfg.batch_size,
      epochs=_EPOCHS,
      validation_data=dataset.make_one_shot_iterator(),
      validation_steps=1000, #len(x_test) // _BATCH_SIZE,
      verbose = 1,
      callbacks=[tensorboard])

  if not os.path.exists("models"):
      os.makedirs("models")
  # serialize model to JSON
  model_json = model.to_json()
  with open("models/model.json", "w") as json_file:
      json_file.write(model_json)
  # serialize weights to HDF5
  model.save_weights("models/model.h5")
  print("Saved model to disk")

  # train loop
  # model.train(
  #   args.learning_rate,
  #   resume=args.resume,
  #   profiling=args.profiling,
  #   summary_step=10)

if __name__ == '__main__':
  parser = argparse.ArgumentParser()
  parser.add_argument('--model', type=str, default='ConvNetQuake')
  parser.add_argument('--checkpoint_dir', type=str, default='output/checkpoints')
  parser.add_argument('--dataset', type=str, default='data/hackathon/train')
  parser.add_argument('--batch_size', type=int, default=64)
  parser.add_argument('--learning_rate', type=float, default=1e-4)
  parser.add_argument('--resume', action='store_true')
  parser.set_defaults(resume=False)
  parser.add_argument('--profiling', action='store_true')
  parser.add_argument('--n_clusters',type=int,default=None)
  parser.set_defaults(profiling=False)
  args = parser.parse_args()

  args.checkpoint_dir = os.path.join(args.checkpoint_dir, args.model)

  main(args)
